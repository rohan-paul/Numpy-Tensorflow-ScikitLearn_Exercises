{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensorflow Issue while model Training - Failed to get convolution algorithm\n",
    "\n",
    "I faced the below issue in my Ubuntu 20.04 machine.\n",
    "\n",
    "#### Error : Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above\n",
    "\n",
    "\n",
    "The solution [here](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-689552885) worked for me.\n",
    "\n",
    "Set the TF_FORCE_GPU_ALLOW_GROWTH environment variable to true.\n",
    "In your terminal, run this command.\n",
    "\n",
    "```\n",
    "export TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "\n",
    "```\n",
    "\n",
    "#### Other Details\n",
    "\n",
    "My cuda version - you will know this by running `nvcc --version``\n",
    "\n",
    "```\n",
    "Cuda compilation tools, release 11.0, V11.0.194\n",
    "Build cuda_11.0_bu.TC445_37.28540450_0\n",
    "```\n",
    "\n",
    "And my Tensorflow version `2.3.1`\n",
    "\n",
    "You can run below command in Terminal to get TF version\n",
    "\n",
    "`python -c 'import tensorflow as tf; print(tf.__version__)'`\n",
    "\n",
    "For explanation on **TF_FORCE_GPU_ALLOW_GROWTH** see [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/gpu.ipynb#scrollTo=ARrRhwqijPzN)\n",
    "\n",
    "\"By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation. To limit TensorFlow to a specific set of GPUs we use the tf.config.experimental.set_visible_devices method.\n",
    "\n",
    "In some cases it is desirable for the process to only allocate a subset of the available memory, or to only grow the memory usage as is needed by the process. TensorFlow provides two methods to control this.\n",
    "\n",
    "The first option is to turn on memory growth by calling tf.config.experimental.set_memory_growth, which attempts to allocate only as much GPU memory as needed for the runtime allocations: it starts out allocating very little memory, and as the program gets run and more GPU memory is needed, we extend the GPU memory region allocated to the TensorFlow process. Note we do not release memory, since it can lead to memory fragmentation.  To turn on memory growth for a specific GPU, use the following code prior to allocating any tensors or executing any ops.\n",
    "\n",
    "\n",
    "```python\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "```\n",
    "\n",
    "#### Another way to enable this option is to set the environmental variable TF_FORCE_GPU_ALLOW_GROWTH to true. This configuration is platform specific.\"\n",
    "\n",
    "The second method is to configure a virtual GPU device with tf.config.experimental.set_virtual_device_configuration and set a hard limit on the total memory to allocate on the GPU.\n",
    "\n",
    "```python\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "```\n",
    "\n",
    "This is useful if you want to truly bound the amount of GPU memory available to the TensorFlow process. This is common practice for local development when the GPU is shared with other applications such as a workstation GUI.\n",
    "\n",
    "---\n",
    "\n",
    "Some further explanation about **TF_FORCE_GPU_ALLOW_GROWTH** from this [Reddit thread](https://www.reddit.com/r/MachineLearning/comments/ea083i/d_tensorflow_gpu_memory_management_tf_force_gpu/)\n",
    "\n",
    "\"The reason for the TF_FORCE_GPU_ALLOW_GROWTH flag is to allow TF to play nice with other apps (or TF instances?) that also need to use GPU memory. The issue is that GPU memory is fundamentally managed by CUDA API's, but for efficiency TF wants to manage the memory itself, so TF maintains it's own heap (memory allocator) using GPU memory it obtained via CUDA, and TF applications then allocate/release memory to/from the TF heap, not directly to/from CUDA.\n",
    "\n",
    "The TF heap only ever grows, when needed (i.e. if a TF app requests more memory than TF currently has available), by grabbing more memory from CUDA. TF never shrinks its heap by releasing memory back to CUDA. The TF_FORCE_GPU_ALLOW_GROWTH flag determines whether TF grabs all the CUDA memory it wants at start-up, or - to play nice with other CUDA apps - starts small and grabs more memory only as needed.\"\n",
    "\n",
    "And the Tensorflow source code of this flag is [here](https://github.com/tensorflow/tensorflow/blob/3e21fe5faedab3a8258d344c8ad1cec2612a8aa8/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc#L25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}